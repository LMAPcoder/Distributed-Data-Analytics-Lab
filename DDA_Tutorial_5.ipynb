{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMddzSYT7LRbaIBVpnz5rdv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Lab Distributed Data Analytics**"
      ],
      "metadata": {
        "id": "uCTM-UseEKqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tutorial 5"
      ],
      "metadata": {
        "id": "9loVfcz_ELws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadoop is an open-source framework which is mainly used for storage purpose and maintaining and analyzing a large amount of data or datasets on the clusters of commodity hardware, which means it is actually a data management tool."
      ],
      "metadata": {
        "id": "RonlTIAFyKPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up a Hadoop infrastructure"
      ],
      "metadata": {
        "id": "cb_zM_G0WJTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and configuring Java"
      ],
      "metadata": {
        "id": "y_WfoYFoWuhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadoop is a java programming-based data processing framework and relies on the Java Virtual Machine (JVM) to run its various components. Hadoop requires a JDK installation with specific versions. For example, Hadoop 3.x is generally compatible with Java 8 or later versions."
      ],
      "metadata": {
        "id": "mU884WMgzKlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing java 8 for compatibility purposes\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "UzvDHnOjWT-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching java version to use as default (option 2)\n",
        "!update-alternatives --config java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHt6yPFpWgS5",
        "outputId": "848e91a0-bcc4-4195-e2f1-f53ce568b1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching javac version to use as default (option 2)\n",
        "!update-alternatives --config javac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9De_838OWkT3",
        "outputId": "9a84740f-90b2-4899-9fb8-89d9dd9c3514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative javac (providing /usr/bin/javac).\n",
            "\n",
            "  Selection    Path                                          Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/javac    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching jps version to use as default (option 2)\n",
        "!update-alternatives --config jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA5QcJFCWota",
        "outputId": "3c9dbac9-806d-4f8d-a7d7-af5f86fdebe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative jps (providing /usr/bin/jps).\n",
            "\n",
            "  Selection    Path                                        Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/jps    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking java default version\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lywpZj7cZ0cH",
        "outputId": "e93e801d-dae7-4b6e-95dc-fdecabd45ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_362\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09)\n",
            "OpenJDK 64-Bit Server VM (build 25.362-b09, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating java home variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += \":$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\""
      ],
      "metadata": {
        "id": "IWmxqYwOWqjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and configuring Secure Shell (SSH)"
      ],
      "metadata": {
        "id": "gX1necayWtPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to define a means for the master node to remotely access every node in our cluster. Hadoop uses passphrases SSH for the communication between the nodes."
      ],
      "metadata": {
        "id": "wAq6vK5WzTdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#It is good practice to purge ssh before installation\n",
        "!apt-get purge openssh-server"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYZKAyiUW7dG",
        "outputId": "f66d0e9e-e989-4575-cea9-a79f765b77f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'openssh-server' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing openssh-server\n",
        "!apt-get install openssh-server -qq > /dev/null"
      ],
      "metadata": {
        "id": "j7AfDJGSW-4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#starting the server\n",
        "!service ssh start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtNieZXXXAHH",
        "outputId": "f84dfa6e-01d4-4605-982b-d9e78bcef469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to make sure that to ssh access to localhost and login we do not need to enter a password. Therefore, SSH needs to be set up to allow passwordless login for the hadoop user. The simplest way to achive this is to generate a public-private key pair."
      ],
      "metadata": {
        "id": "oV8xw6Nb2wfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4skpCz2yXDLj",
        "outputId": "bdd16060-ebc1-4c01-cb1a-b2cb475e3b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:OpVr+4wgJFUOV0i4kr948BnY9Xlx0uDFijl+DgSGhEs root@7ae669598d5a\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|   oo++o.  .     |\n",
            "|  E o=+   . o    |\n",
            "| . o.o.. + =     |\n",
            "|  +.. . =.= o    |\n",
            "|  .=.. +So +     |\n",
            "|  oo+  o=.o      |\n",
            "|   +.++ o=       |\n",
            "|  . =. + +.      |\n",
            "|   .    o.o      |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Showing the public key\n",
        "!more /root/.ssh/id_rsa.pub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVWqJTxcXExE",
        "outputId": "9500c07c-f75d-4db8-89f1-d68106ab1b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDc7AOtSCpu5s9c/pemtsbtH3FdQTLJ2nWy7yJf0aMg\n",
            "bas6JLtbHCxVuhl3Bnk5jS91ZqiYPb4yaXgmME25WPkHnLglo4LbfIUu/AxwC3U7IbzEJxkJz8QDm+Ok\n",
            "nEn+UgvOSRa0MOJyh950sDy4yja7fKbpKxVdDY0w8PqZrEY8l5QQpkceIC/k+dt55OYLJxl7vIO89VCZ\n",
            "kXYCvoeFyKT8Yi36uk91ULVK4z/GrVhUWkYjGXpd/nRAcK1Bs3J9K3GMjBd/YpChKHQ1PHxlbSioP+NY\n",
            "iYpyiOnoz7ac9iLvrB9/okFu1AAfdNkheGGk3HIeo14jl6/CJGV7TvvjmUd/9IskAxwaOLiUCHHD8+X+\n",
            "Azuj98nfTh/3SxfzTvj6fsiAo8NC/2tgConrE9H1Xd5hLxVCH8/ViGCrQ0/HUUr35owvQMJHK9Cm9jKC\n",
            "Rw6lza7JVQ1pQ6x30o3zwJVKQoPYTlXu9mnmxRDz0zH8b2sG0aexJs2y0/VB/nP14Vk2bOE= root@7a\n",
            "e669598d5a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
        "#changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "id": "WGtU_r1JXGB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RdBvxfkXIuy",
        "outputId": "1ca5c02c-3b08-4580-e408-2bd46e1321eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\r\n",
            " 08:46:08 up 4 min,  0 users,  load average: 1.10, 0.60, 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Hadoop 3.2.3"
      ],
      "metadata": {
        "id": "TN1FVzfxXLft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading Hadoop 3.2.3\n",
        "#From Google drive\n",
        "!gdown 'https://drive.google.com/uc?id=12P5hpS2DjMG4P3YukBP0D4s6uUUEJG-A' -O hadoop-3.2.3.tar.gz\n",
        "# !wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz #From official website"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uudwiicHXMHL",
        "outputId": "46aabe44-e214-4d6d-86f0-bd2f5d5ac5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12P5hpS2DjMG4P3YukBP0D4s6uUUEJG-A\n",
            "To: /content/hadoop-3.2.3.tar.gz\n",
            "100% 492M/492M [00:07<00:00, 65.2MB/s]\n",
            "CPU times: user 91.1 ms, sys: 12 ms, total: 103 ms\n",
            "Wall time: 8.87 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "!rm hadoop-3.2.3.tar.gz #to remove the tar file"
      ],
      "metadata": {
        "id": "9biLUuRjXNdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copying the hadoop file to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/"
      ],
      "metadata": {
        "id": "ADIyWr3bXOow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop-3.2.3 directory\n",
        "!ls -l /usr/local/hadoop-3.2.3/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV5zdW3PXPzG",
        "outputId": "3f61e8b3-5777-414f-a5fd-a310193831de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 204\n",
            "drwxr-xr-x 2 root root   4096 May 27 08:46 bin\n",
            "drwxr-xr-x 3 root root   4096 May 27 08:46 etc\n",
            "drwxr-xr-x 2 root root   4096 May 27 08:46 include\n",
            "drwxr-xr-x 3 root root   4096 May 27 08:46 lib\n",
            "drwxr-xr-x 4 root root   4096 May 27 08:46 libexec\n",
            "-rw-r--r-- 1 root root 150571 May 27 08:46 LICENSE.txt\n",
            "-rw-r--r-- 1 root root  21943 May 27 08:46 NOTICE.txt\n",
            "-rw-r--r-- 1 root root   1361 May 27 08:46 README.txt\n",
            "drwxr-xr-x 3 root root   4096 May 27 08:46 sbin\n",
            "drwxr-xr-x 4 root root   4096 May 27 08:46 share\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Specifing the JAVA_HOME variable in hadoop-env.sh\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "WAvpEAbeXS6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating hadoop home variable\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\""
      ],
      "metadata": {
        "id": "PfntLQeiXUUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadoop modes"
      ],
      "metadata": {
        "id": "IsVdwE7byV0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadoop Mainly works on 3 different Modes:\n",
        "\n",
        "*   Standalone Mode\n",
        "*   Pseudo-distributed Mode\n",
        "*   Fully-Distributed Mode"
      ],
      "metadata": {
        "id": "Wo5scXu6yP_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standalone Mode\n",
        "\n",
        "By default, Hadoop is configured to run in a no distributed mode. It runs as a single Java process. Instead of HDFS, this mode utilizes the local file system. This mode useful for debugging and there isn't any need to configure core-site.xml, hdfs-site.xml, mapred-site.xml, masters & slaves. Stand-alone mode is usually the fastest mode in Hadoop.\n",
        "\n",
        "Pseudo-distributed Mode\n",
        "\n",
        "Hadoop can also run on a single node in a Pseudo Distributed mode. In this mode, each daemon runs on separate java process. In this mode custom configuration is required( core-site.xml, hdfs-site.xml, mapred-site.xml). Here HDFS is utilized for input and output. This mode of deployment is useful for testing and debugging purposes.\n",
        "\n",
        "Fully Distributed Mode\n",
        "\n",
        "This is the production mode of Hadoop. In this mode typically one machine in the cluster is designated as NameNode and another as Resource Manager exclusively. These are masters. All other nodes act as Data Node and Node Manager. These are the slaves. Configuration parameters and environment need to specified for Hadoop Daemons."
      ],
      "metadata": {
        "id": "jlf1OUjhycOr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running Hadoop in Pseudo-distributed mode"
      ],
      "metadata": {
        "id": "PVYfGUW4XWjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the distributed components of Hadoop comes into play. That is, all the Hadoop deamons that are responsible for distributed storage and distributed processing will run on the same machine.\n",
        "\n",
        "Master deamons:\n",
        "\n",
        "*   NameNode\n",
        "*   Resource Manager\n",
        "*   Standby NameNode\n",
        "\n",
        "Slave deamons:\n",
        "\n",
        "*   DataNode\n",
        "*   Node Manager"
      ],
      "metadata": {
        "id": "2VaNy0Io0Hq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By setting the properties in these xml configuration files, we tell hadoop which machines are in the cluster and where and how we want to run the hadoop deamons"
      ],
      "metadata": {
        "id": "dA-0pK_t0Omp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring core-site.xml\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>fs.defaultFS</name>\\n\\\n",
        "    <value>hdfs://localhost:9000</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "id": "hvGPbAY7XW--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring hdfs-site.xml\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>dfs.replication</name>\\n\\\n",
        "    <value>1</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "id": "YJSzzkTHXYTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring mapred-site.xml\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.framework.name</name>\\n\\\n",
        "    <value>yarn</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.application.classpath</name>\\n\\\n",
        "    <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "id": "MCVXO6hkXamU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring yarn-site.xml\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <description>The hostname of the RM.</description>\\n\\\n",
        "    <name>yarn.resourcemanager.hostname</name>\\n\\\n",
        "    <value>localhost</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.aux-services</name>\\n\\\n",
        "    <value>mapreduce_shuffle</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.env-whitelist</name>\\n\\\n",
        "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "id": "BaAl7CyDXb2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before HDFS can be used for the first time the file system must be formatted."
      ],
      "metadata": {
        "id": "9SJpKGJU0U-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Formatting to delete namenode mata data\n",
        "!$HADOOP_HOME/bin/hdfs namenode -format"
      ],
      "metadata": {
        "id": "-zju8JYGXf2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ef62da-a806-4787-a1b3-28b5db4a6487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: /usr/local/hadoop-3.2.3/logs does not exist. Creating.\n",
            "2023-05-27 08:47:00,866 INFO namenode.NameNode: STARTUP_MSG: \n",
            "/************************************************************\n",
            "STARTUP_MSG: Starting NameNode\n",
            "STARTUP_MSG:   host = 7ae669598d5a/172.28.0.12\n",
            "STARTUP_MSG:   args = [-format]\n",
            "STARTUP_MSG:   version = 3.2.3\n",
            "STARTUP_MSG:   classpath = /usr/local/hadoop-3.2.3/etc/hadoop:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-kms-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/common/hadoop-common-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-servlet-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.activation-api-1.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-security-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/zookeeper-3.4.14.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-compress-1.21.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/stax2-api-4.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/spotbugs-annotations-3.1.9.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr305-3.0.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-beanutils-1.9.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-databind-2.10.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-annotations-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsch-0.1.55.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-xml-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpcore-4.4.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/nimbus-jose-jwt-9.8.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-http-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/accessors-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/httpclient-4.5.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-3.10.6.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-util-ajax-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/netty-all-4.1.68.Final.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-smart-2.4.7.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/woodstox-core-5.3.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-webapp-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/hadoop-auth-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-server-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/commons-io-2.8.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/lib/jetty-io-9.4.40.v20210413.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/lib/junit-4.13.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3-tests.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/fst-2.50.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/snakeyaml-1.26.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.activation-api-1.2.1.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jakarta.xml.bind-api-2.3.2.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/lib/jackson-jaxrs-base-2.10.5.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-core-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-services-api-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-router-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-client-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-submarine-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-registry-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-common-3.2.3.jar:/usr/local/hadoop-3.2.3/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.3.jar\n",
            "STARTUP_MSG:   build = https://github.com/apache/hadoop -r abe5358143720085498613d399be3bbf01e0f131; compiled by 'ubuntu' on 2022-03-20T01:18Z\n",
            "STARTUP_MSG:   java = 1.8.0_362\n",
            "************************************************************/\n",
            "2023-05-27 08:47:00,921 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
            "2023-05-27 08:47:01,061 INFO namenode.NameNode: createNameNode [-format]\n",
            "Formatting using clusterid: CID-ab25879f-4deb-4c32-9359-bcea65733f7e\n",
            "2023-05-27 08:47:01,823 INFO namenode.FSEditLog: Edit logging is async:true\n",
            "2023-05-27 08:47:01,864 INFO namenode.FSNamesystem: KeyProvider: null\n",
            "2023-05-27 08:47:01,867 INFO namenode.FSNamesystem: fsLock is fair: true\n",
            "2023-05-27 08:47:01,868 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
            "2023-05-27 08:47:01,876 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
            "2023-05-27 08:47:01,876 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
            "2023-05-27 08:47:01,877 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
            "2023-05-27 08:47:01,877 INFO namenode.FSNamesystem: HA Enabled: false\n",
            "2023-05-27 08:47:01,940 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
            "2023-05-27 08:47:01,952 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
            "2023-05-27 08:47:01,952 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
            "2023-05-27 08:47:01,970 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
            "2023-05-27 08:47:01,971 INFO blockmanagement.BlockManager: The block deletion will start around 2023 May 27 08:47:01\n",
            "2023-05-27 08:47:01,973 INFO util.GSet: Computing capacity for map BlocksMap\n",
            "2023-05-27 08:47:01,974 INFO util.GSet: VM type       = 64-bit\n",
            "2023-05-27 08:47:01,975 INFO util.GSet: 2.0% max memory 2.8 GB = 57.7 MB\n",
            "2023-05-27 08:47:01,975 INFO util.GSet: capacity      = 2^23 = 8388608 entries\n",
            "2023-05-27 08:47:01,988 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\n",
            "2023-05-27 08:47:01,988 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\n",
            "2023-05-27 08:47:01,995 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
            "2023-05-27 08:47:01,995 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
            "2023-05-27 08:47:01,995 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
            "2023-05-27 08:47:01,996 INFO blockmanagement.BlockManager: defaultReplication         = 1\n",
            "2023-05-27 08:47:01,996 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
            "2023-05-27 08:47:01,996 INFO blockmanagement.BlockManager: minReplication             = 1\n",
            "2023-05-27 08:47:01,996 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
            "2023-05-27 08:47:01,996 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\n",
            "2023-05-27 08:47:01,996 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
            "2023-05-27 08:47:01,996 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
            "2023-05-27 08:47:02,022 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\n",
            "2023-05-27 08:47:02,023 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\n",
            "2023-05-27 08:47:02,023 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\n",
            "2023-05-27 08:47:02,023 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\n",
            "2023-05-27 08:47:02,040 INFO util.GSet: Computing capacity for map INodeMap\n",
            "2023-05-27 08:47:02,040 INFO util.GSet: VM type       = 64-bit\n",
            "2023-05-27 08:47:02,040 INFO util.GSet: 1.0% max memory 2.8 GB = 28.9 MB\n",
            "2023-05-27 08:47:02,040 INFO util.GSet: capacity      = 2^22 = 4194304 entries\n",
            "2023-05-27 08:47:02,094 INFO namenode.FSDirectory: ACLs enabled? false\n",
            "2023-05-27 08:47:02,094 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\n",
            "2023-05-27 08:47:02,094 INFO namenode.FSDirectory: XAttrs enabled? true\n",
            "2023-05-27 08:47:02,095 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
            "2023-05-27 08:47:02,101 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\n",
            "2023-05-27 08:47:02,103 INFO snapshot.SnapshotManager: SkipList is disabled\n",
            "2023-05-27 08:47:02,108 INFO util.GSet: Computing capacity for map cachedBlocks\n",
            "2023-05-27 08:47:02,108 INFO util.GSet: VM type       = 64-bit\n",
            "2023-05-27 08:47:02,108 INFO util.GSet: 0.25% max memory 2.8 GB = 7.2 MB\n",
            "2023-05-27 08:47:02,108 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
            "2023-05-27 08:47:02,116 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
            "2023-05-27 08:47:02,117 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
            "2023-05-27 08:47:02,117 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
            "2023-05-27 08:47:02,121 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
            "2023-05-27 08:47:02,121 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
            "2023-05-27 08:47:02,123 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
            "2023-05-27 08:47:02,123 INFO util.GSet: VM type       = 64-bit\n",
            "2023-05-27 08:47:02,123 INFO util.GSet: 0.029999999329447746% max memory 2.8 GB = 886.4 KB\n",
            "2023-05-27 08:47:02,124 INFO util.GSet: capacity      = 2^17 = 131072 entries\n",
            "2023-05-27 08:47:02,155 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1187038897-172.28.0.12-1685177222145\n",
            "2023-05-27 08:47:02,183 INFO common.Storage: Storage directory /tmp/hadoop-root/dfs/name has been successfully formatted.\n",
            "2023-05-27 08:47:02,224 INFO namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 using no compression\n",
            "2023-05-27 08:47:02,340 INFO namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-root/dfs/name/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\n",
            "2023-05-27 08:47:02,390 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
            "2023-05-27 08:47:02,436 INFO namenode.FSNamesystem: Stopping services started for active state\n",
            "2023-05-27 08:47:02,437 INFO namenode.FSNamesystem: Stopping services started for standby state\n",
            "2023-05-27 08:47:02,441 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\n",
            "2023-05-27 08:47:02,441 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
            "/************************************************************\n",
            "SHUTDOWN_MSG: Shutting down NameNode at 7ae669598d5a/172.28.0.12\n",
            "************************************************************/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hadoop scripts\n",
        "!ls $HADOOP_HOME/sbin"
      ],
      "metadata": {
        "id": "weyG9jY7XiQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba3dc4f-2922-48d2-8a35-219ce6a29d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distribute-exclude.sh\t start-all.sh\t      stop-balancer.sh\n",
            "FederationStateStore\t start-balancer.sh    stop-dfs.cmd\n",
            "hadoop-daemon.sh\t start-dfs.cmd\t      stop-dfs.sh\n",
            "hadoop-daemons.sh\t start-dfs.sh\t      stop-secure-dns.sh\n",
            "httpfs.sh\t\t start-secure-dns.sh  stop-yarn.cmd\n",
            "kms.sh\t\t\t start-yarn.cmd       stop-yarn.sh\n",
            "mr-jobhistory-daemon.sh  start-yarn.sh\t      workers.sh\n",
            "refresh-namenodes.sh\t stop-all.cmd\t      yarn-daemon.sh\n",
            "start-all.cmd\t\t stop-all.sh\t      yarn-daemons.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating other necessary enviromenal variables\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\""
      ],
      "metadata": {
        "id": "IlSeuieeXjpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#starting dfs nodes\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh"
      ],
      "metadata": {
        "id": "gMT5N7lEXlOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b24425-2499-4074-91a1-8c47d9ecb930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [7ae669598d5a]\n",
            "7ae669598d5a: Warning: Permanently added '7ae669598d5a,172.28.0.12' (ECDSA) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#starting yarn nodes\n",
        "!$HADOOP_HOME/sbin/start-yarn.sh"
      ],
      "metadata": {
        "id": "t3IJM4mRXonO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb37f26-876e-41f1-e1af-80067f9c0cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting resourcemanager\n",
            "Starting nodemanagers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#listing the deamons that are running\n",
        "!jps"
      ],
      "metadata": {
        "id": "kikj2aUlXpjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3c65d0-3db2-4718-9556-af823387bd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2753 NodeManager\n",
            "2642 ResourceManager\n",
            "2389 SecondaryNameNode\n",
            "2886 Jps\n",
            "2201 DataNode\n",
            "2093 NameNode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expose localhost on a server"
      ],
      "metadata": {
        "id": "JVxHDuw0XxZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output"
      ],
      "metadata": {
        "id": "wBYUS5NoXufU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Access hadoop web browser interface\n",
        "output.serve_kernel_port_as_window(9870)"
      ],
      "metadata": {
        "id": "HpSI4NkoXyah",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eed2d2f1-e685-48f4-9686-2abe3fd587b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(9870, \"/\", \"https://localhost:9870/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading and placing the file in HFDS"
      ],
      "metadata": {
        "id": "EOGE_pUwYHVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting file from Google drive\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1nYWcznOsh1dcpMJxgP0nZzKGuB83Awp6' -O adjacency.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G7QexmULZoF",
        "outputId": "f28bb108-926d-4302-96d2-bb8a73238f16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-27 08:47:41--  https://docs.google.com/uc?export=download&id=1nYWcznOsh1dcpMJxgP0nZzKGuB83Awp6\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.139, 142.251.2.113, 142.251.2.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ao-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sqjbhbmiksit8v39nto6cd834b2hhart/1685177250000/14760575472933726065/*/1nYWcznOsh1dcpMJxgP0nZzKGuB83Awp6?e=download&uuid=64225266-aaa7-47f3-b2c3-6c0b9b9a3af7 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-27 08:47:41--  https://doc-04-ao-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/sqjbhbmiksit8v39nto6cd834b2hhart/1685177250000/14760575472933726065/*/1nYWcznOsh1dcpMJxgP0nZzKGuB83Awp6?e=download&uuid=64225266-aaa7-47f3-b2c3-6c0b9b9a3af7\n",
            "Resolving doc-04-ao-docs.googleusercontent.com (doc-04-ao-docs.googleusercontent.com)... 142.250.141.132, 2607:f8b0:4023:c0b::84\n",
            "Connecting to doc-04-ao-docs.googleusercontent.com (doc-04-ao-docs.googleusercontent.com)|142.250.141.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84 [text/plain]\n",
            "Saving to: adjacency.txt\n",
            "\n",
            "adjacency.txt       100%[===================>]      84  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-27 08:47:42 (2.82 MB/s) - adjacency.txt saved [84/84]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a directory within dfs\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /node_count_in_python"
      ],
      "metadata": {
        "id": "fcSVKbCXYMD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting the file from local file system to hadoop distributed file system\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/adjacency.txt /node_count_in_python"
      ],
      "metadata": {
        "id": "m-FX202zYRyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /node_count_in_python"
      ],
      "metadata": {
        "id": "6UtWuJqoYWXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91fd87c-9bc8-48e0-f887-e99918045826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup         84 2023-05-27 08:47 /node_count_in_python/adjacency.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mapper and reducer for counting neighbors in a graph"
      ],
      "metadata": {
        "id": "GX-j3A-mWJ8k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3P83CH4GtHJ",
        "outputId": "542ab002-b9d4-4ac1-d481-14c3bc84151f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ex2_mapper.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ex2_mapper.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "import sys\n",
        "i = 0\n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "  # to remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # split the row into values\n",
        "  node = int(line[0])\n",
        "  adj_vector = line[2:].split(',')\n",
        "\n",
        "  for _ in adj_vector:\n",
        "    # write the results to STDOUT (standard output);\n",
        "    print('%s\\t%s' % (node, _)) #Hadoop uses tab(\\t) as a default separator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ex2_reducer.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "current_node = None\n",
        "current_count = 0\n",
        "node = None\n",
        "\n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "  # remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # splitting the data on the basis of tab we have provided in mapper.py\n",
        "  node, count = line.split('\\t', 1) #maxsplit = 1\n",
        "\n",
        "  # convert count (currently a string) to int\n",
        "  try:\n",
        "    count = int(count)\n",
        "  except ValueError:\n",
        "    # count was not a number, so silently\n",
        "    # ignore/discard this line\n",
        "    continue\n",
        "\n",
        "  # this IF-switch only works because Hadoop sorts map output\n",
        "  # by key (here: word) before it is passed to the reducer\n",
        "  if current_node == node:\n",
        "    current_count += count\n",
        "  else:\n",
        "    if current_node: #to not print current_word=None\n",
        "      # write result to STDOUT\n",
        "      print('%s\\t%s' % (current_node, current_count))\n",
        "    current_count = count\n",
        "    current_node = node\n",
        "\n",
        "#output of last node\n",
        "if current_node == node:\n",
        "  print('%s\\t%s' % (current_node, current_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMLSo1lMIzE0",
        "outputId": "d2862c74-8d7b-445f-c156-0830ce481c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ex2_reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the python files work properly (Hadoop is not involved)\n",
        "!cat adjacency.txt | python ex2_mapper.py | sort -k1,1 | python ex2_reducer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHm6UmaYIh1-",
        "outputId": "afafdc21-8a38-424e-a0d0-cffcf58364d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t3\n",
            "2\t2\n",
            "3\t3\n",
            "4\t2\n",
            "5\t4\n",
            "6\t2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the python files\n",
        "!chmod 777 /content/ex2_mapper.py /content/ex2_reducer.py"
      ],
      "metadata": {
        "id": "IV_bZm7SYuBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadoop Streaming is a feature that comes with Hadoop and allows users or developers to use various different languages for writing MapReduce programs like Python, C++, Ruby, etc."
      ],
      "metadata": {
        "id": "l751q2T00pSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Running hadoop streaming\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input /node_count_in_python/adjacency.txt \\\n",
        "  -output /node_count_in_python/output \\\n",
        "  -mapper \"python /content/ex2_mapper.py\" \\\n",
        "  -reducer \"python /content/ex2_reducer.py\""
      ],
      "metadata": {
        "id": "LIEqvQDQYyBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716d9546-36a3-4d49-8d3a-02b42cd747a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar3347244383225620917/] [] /tmp/streamjob7091167848872007985.jar tmpDir=null\n",
            "2023-05-27 08:48:08,488 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-27 08:48:08,800 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-27 08:48:09,145 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1685177260195_0001\n",
            "2023-05-27 08:48:09,515 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-05-27 08:48:10,462 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2023-05-27 08:48:10,715 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1685177260195_0001\n",
            "2023-05-27 08:48:10,719 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-05-27 08:48:11,016 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-05-27 08:48:11,017 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-05-27 08:48:11,489 INFO impl.YarnClientImpl: Submitted application application_1685177260195_0001\n",
            "2023-05-27 08:48:11,545 INFO mapreduce.Job: The url to track the job: http://7ae669598d5a:8088/proxy/application_1685177260195_0001/\n",
            "2023-05-27 08:48:11,547 INFO mapreduce.Job: Running job: job_1685177260195_0001\n",
            "2023-05-27 08:48:23,832 INFO mapreduce.Job: Job job_1685177260195_0001 running in uber mode : false\n",
            "2023-05-27 08:48:23,834 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-05-27 08:48:33,995 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-05-27 08:48:40,049 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-05-27 08:48:41,065 INFO mapreduce.Job: Job job_1685177260195_0001 completed successfully\n",
            "2023-05-27 08:48:41,160 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=222\n",
            "\t\tFILE: Number of bytes written=715832\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=342\n",
            "\t\tHDFS: Number of bytes written=24\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=16378\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=3399\n",
            "\t\tTotal time spent by all map tasks (ms)=16378\n",
            "\t\tTotal time spent by all reduce tasks (ms)=3399\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=16378\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=3399\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=16771072\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3480576\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6\n",
            "\t\tMap output records=36\n",
            "\t\tMap output bytes=144\n",
            "\t\tMap output materialized bytes=228\n",
            "\t\tInput split bytes=216\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=6\n",
            "\t\tReduce shuffle bytes=228\n",
            "\t\tReduce input records=36\n",
            "\t\tReduce output records=6\n",
            "\t\tSpilled Records=72\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=762\n",
            "\t\tCPU time spent (ms)=2580\n",
            "\t\tPhysical memory (bytes) snapshot=823181312\n",
            "\t\tVirtual memory (bytes) snapshot=7617622016\n",
            "\t\tTotal committed heap usage (bytes)=841482240\n",
            "\t\tPeak Map Physical memory (bytes)=321658880\n",
            "\t\tPeak Map Virtual memory (bytes)=2537775104\n",
            "\t\tPeak Reduce Physical memory (bytes)=222048256\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2543280128\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=126\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=24\n",
            "2023-05-27 08:48:41,160 INFO streaming.StreamJob: Output directory: /node_count_in_python/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop folder\n",
        "# !$HADOOP_HOME/bin/hdfs dfs -ls /node_count_in_python/output\n",
        "!$HADOOP_HOME/bin/hdfs dfs -rm -r /node_count_in_python/output"
      ],
      "metadata": {
        "id": "BOkecX4tZbFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0079c41-3710-4ee8-f1dd-fb9baa83f558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted /node_count_in_python/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the output from hadoop file system\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /node_count_in_python/output/part-00000"
      ],
      "metadata": {
        "id": "BNapztD7Zdcv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66dccdb4-bbb4-422b-8c39-9175f294348c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t3\n",
            "2\t2\n",
            "3\t3\n",
            "4\t2\n",
            "5\t4\n",
            "6\t2\n"
          ]
        }
      ]
    }
  ]
}