{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7Fuv8+N0F4g3Bbn8b27I8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab Distributed Data Analytics**"
      ],
      "metadata": {
        "id": "uCTM-UseEKqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial 6"
      ],
      "metadata": {
        "id": "9loVfcz_ELws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up Hadoop infrastructure"
      ],
      "metadata": {
        "id": "qAchjS8BIAgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and configuring Java"
      ],
      "metadata": {
        "id": "y_WfoYFoWuhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing java 8 for compatibility purposes\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "UzvDHnOjWT-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching java version to use as default (option 2)\n",
        "!update-alternatives --config java"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHt6yPFpWgS5",
        "outputId": "4d210ab7-1f23-44a9-f379-2e7083d98f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching javac version to use as default (option 2)\n",
        "!update-alternatives --config javac"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9De_838OWkT3",
        "outputId": "cc13270f-ff59-45cb-9d91-954ad0a9c5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative javac (providing /usr/bin/javac).\n",
            "\n",
            "  Selection    Path                                          Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/javac   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/javac    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javac to provide /usr/bin/javac (javac) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Switching jps version to use as default (option 2)\n",
        "!update-alternatives --config jps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WA5QcJFCWota",
        "outputId": "4d0118b0-bff8-43ac-873a-1c03f0dba4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 choices for the alternative jps (providing /usr/bin/jps).\n",
            "\n",
            "  Selection    Path                                        Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/jps   1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/bin/jps    1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jps to provide /usr/bin/jps (jps) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking java default version\n",
        "!java -version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lywpZj7cZ0cH",
        "outputId": "b9e49c06-6ff3-45f3-dfe4-77c8f6b653a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_362\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_362-8u372-ga~us1-0ubuntu1~20.04-b09)\n",
            "OpenJDK 64-Bit Server VM (build 25.362-b09, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating java home variable\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"JRE_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64/jre\"\n",
        "os.environ[\"PATH\"] += \":$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\""
      ],
      "metadata": {
        "id": "IWmxqYwOWqjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and configuring Secure Shell (SSH)"
      ],
      "metadata": {
        "id": "gX1necayWtPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#It is good practice to purge ssh before installation\n",
        "!apt-get purge openssh-server"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYZKAyiUW7dG",
        "outputId": "8eb53fbe-1919-4248-d652-a230a9c907e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Package 'openssh-server' is not installed, so not removed\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing openssh-server\n",
        "!apt-get install openssh-server -qq > /dev/null"
      ],
      "metadata": {
        "id": "j7AfDJGSW-4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#starting the server\n",
        "!service ssh start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtNieZXXXAHH",
        "outputId": "d349f5d6-ec50-47da-928a-6bb675dabee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting OpenBSD Secure Shell server sshd\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new rsa key pair with empty password\n",
        "!ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4skpCz2yXDLj",
        "outputId": "4102cd9f-b891-45d3-8e52-3f1562c1fc9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private rsa key pair.\n",
            "Created directory '/root/.ssh'.\n",
            "Your identification has been saved in /root/.ssh/id_rsa\n",
            "Your public key has been saved in /root/.ssh/id_rsa.pub\n",
            "The key fingerprint is:\n",
            "SHA256:UMDopko0yqoVa8g+FyEWTa4XdQ/uqgG+Hb3Nqzcg58I root@de8bf04d0ad8\n",
            "The key's randomart image is:\n",
            "+---[RSA 3072]----+\n",
            "|  o. +.+.        |\n",
            "| ...o +.o        |\n",
            "|  .+  .. .       |\n",
            "| =..+ ..         |\n",
            "|=o++.  .S        |\n",
            "|+++=.o.          |\n",
            "|++=o=o.          |\n",
            "|++oE+.+o         |\n",
            "|oooo.oo+o        |\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#copying the key to autorized keys\n",
        "!cat $HOME/.ssh/id_rsa.pub>>$HOME/.ssh/authorized_keys\n",
        "#changing the permissions on the key\n",
        "!chmod 0600 ~/.ssh/authorized_keys"
      ],
      "metadata": {
        "id": "WGtU_r1JXGB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#conneting with the local machine\n",
        "!ssh -o StrictHostKeyChecking=no localhost uptime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RdBvxfkXIuy",
        "outputId": "db2649d0-cfc4-425f-eb43-58343c753bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Permanently added 'localhost' (ECDSA) to the list of known hosts.\r\n",
            " 08:12:25 up 7 min,  0 users,  load average: 1.02, 0.47, 0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Hadoop 3.2.3"
      ],
      "metadata": {
        "id": "TN1FVzfxXLft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading Hadoop 3.2.3\n",
        "#From Google drive\n",
        "!gdown 'https://drive.google.com/uc?id=12P5hpS2DjMG4P3YukBP0D4s6uUUEJG-A' -O hadoop-3.2.3.tar.gz\n",
        "# !wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz #From official website"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uudwiicHXMHL",
        "outputId": "dc2a83fc-3618-4687-8e6c-ebf58e0c5e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12P5hpS2DjMG4P3YukBP0D4s6uUUEJG-A\n",
            "To: /content/hadoop-3.2.3.tar.gz\n",
            "100% 492M/492M [00:04<00:00, 113MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#untarring the file\n",
        "!sudo tar -xzf hadoop-3.2.3.tar.gz\n",
        "!rm hadoop-3.2.3.tar.gz #to remove the tar file"
      ],
      "metadata": {
        "id": "9biLUuRjXNdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copying the hadoop file to user/local\n",
        "!cp -r hadoop-3.2.3/ /usr/local/"
      ],
      "metadata": {
        "id": "ADIyWr3bXOow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Specifing the JAVA_HOME variable in hadoop-env.sh\n",
        "!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\\/usr\\/lib\\/jvm\\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh"
      ],
      "metadata": {
        "id": "WAvpEAbeXS6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating hadoop home variable\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.2.3\""
      ],
      "metadata": {
        "id": "PfntLQeiXUUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running Hadoop in Pseudo-distributed mode"
      ],
      "metadata": {
        "id": "ZBjK8_XPKNuL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring core-site.xml\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>fs.defaultFS</name>\\n\\\n",
        "    <value>hdfs://localhost:9000</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/core-site.xml"
      ],
      "metadata": {
        "id": "hvGPbAY7XW--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring hdfs-site.xml\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>dfs.replication</name>\\n\\\n",
        "    <value>1</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/hdfs-site.xml"
      ],
      "metadata": {
        "id": "YJSzzkTHXYTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring mapred-site.xml\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.framework.name</name>\\n\\\n",
        "    <value>yarn</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>mapreduce.application.classpath</name>\\n\\\n",
        "    <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/mapred-site.xml"
      ],
      "metadata": {
        "id": "MCVXO6hkXamU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuring yarn-site.xml\n",
        "!sed -i '/<configuration>/a\\\n",
        "  <property>\\n\\\n",
        "    <description>The hostname of the RM.</description>\\n\\\n",
        "    <name>yarn.resourcemanager.hostname</name>\\n\\\n",
        "    <value>localhost</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.aux-services</name>\\n\\\n",
        "    <value>mapreduce_shuffle</value>\\n\\\n",
        "  </property>\\n\\\n",
        "  <property>\\n\\\n",
        "    <name>yarn.nodemanager.env-whitelist</name>\\n\\\n",
        "    <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME</value>\\n\\\n",
        "  </property>' \\\n",
        "$HADOOP_HOME/etc/hadoop/yarn-site.xml"
      ],
      "metadata": {
        "id": "BaAl7CyDXb2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Formatting to delete namenode mata data\n",
        "!$HADOOP_HOME/bin/hdfs namenode -format"
      ],
      "metadata": {
        "id": "-zju8JYGXf2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating other necessary enviromenal variables\n",
        "os.environ[\"HDFS_NAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_DATANODE_USER\"] = \"root\"\n",
        "os.environ[\"HDFS_SECONDARYNAMENODE_USER\"] = \"root\"\n",
        "os.environ[\"YARN_RESOURCEMANAGER_USER\"] = \"root\"\n",
        "os.environ[\"YARN_NODEMANAGER_USER\"] = \"root\""
      ],
      "metadata": {
        "id": "IlSeuieeXjpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#starting dfs nodes\n",
        "!$HADOOP_HOME/sbin/start-dfs.sh\n",
        "# !$HADOOP_HOME/sbin/stop-dfs.sh"
      ],
      "metadata": {
        "id": "gMT5N7lEXlOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bff996c-8269-46b3-ab56-a6720ca09294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting namenodes on [localhost]\n",
            "Starting datanodes\n",
            "Starting secondary namenodes [de8bf04d0ad8]\n",
            "de8bf04d0ad8: Warning: Permanently added 'de8bf04d0ad8,172.28.0.12' (ECDSA) to the list of known hosts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#starting yarn nodes\n",
        "!$HADOOP_HOME/sbin/start-yarn.sh\n",
        "# !$HADOOP_HOME/sbin/stop-yarn.sh"
      ],
      "metadata": {
        "id": "t3IJM4mRXonO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f309a23-2266-4108-a558-7ad8a352d756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting resourcemanager\n",
            "Starting nodemanagers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#listing the deamons that are running\n",
        "!jps"
      ],
      "metadata": {
        "id": "kikj2aUlXpjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eaa6ae6-690a-4854-a8af-99349a2e7923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3136 SecondaryNameNode\n",
            "3476 NodeManager\n",
            "2825 NameNode\n",
            "3754 Jps\n",
            "2941 DataNode\n",
            "3358 ResourceManager\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a directory within dfs\n",
        "!$HADOOP_HOME/bin/hdfs dfs -mkdir /TextRank"
      ],
      "metadata": {
        "id": "fcSVKbCXYMD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TextRank"
      ],
      "metadata": {
        "id": "A_mFx4BvISih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **TextRank** summarization algorithm internally uses the popular **PageRank** algorithm, which is used by Google for ranking web sites and pages.\n",
        "The core algorithm in PageRank is a graph-based scoring or ranking algorithm, where pages are scored or ranked based on their importance. Web sites and pages contain further links embedded in them, which link to more pages with more links. This can be represented as a graph-based model where vertices indicate the web pages, and edges indicate links among them. This can be used to form a *voting* or recommendation system.\n",
        "\n",
        "In TextRank algorithm the vertices are sentences, keywords, or phrases. The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph. Any relation that can be defined between two lexical units is a potentially useful connection (edge). In this notebook we use a **co-occurrence relation**: two vertices are connected if their corresponding lexical units co-occur within a window of maximum N words, where N can be set anywhere from 2 to 10 words.\n",
        "\n",
        "The vertices added to the graph can be restricted with **syntactic filters**, which select only certain lexical units. In this notebook we remove *stopwords*."
      ],
      "metadata": {
        "id": "1vxS-4SFLGZG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz23VKNzfSJT",
        "outputId": "b105af04-448b-4b29-b9b0-333a8471338f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-29 08:13:14--  https://www.gutenberg.org/files/2701/2701-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1276235 (1.2M) [text/plain]\n",
            "Saving to: ‘2701-0.txt’\n",
            "\n",
            "2701-0.txt          100%[===================>]   1.22M  6.64MB/s    in 0.2s    \n",
            "\n",
            "2023-05-29 08:13:14 (6.64 MB/s) - ‘2701-0.txt’ saved [1276235/1276235]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Getting file from Google drive\n",
        "!wget --no-check-certificate 'https://www.gutenberg.org/files/2701/2701-0.txt' -O 2701-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -c 500 2701-0.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Jz6-tWcgeJk",
        "outputId": "592a5c55-3c62-4ea3-9dc5-cf632e43fb00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿The Project Gutenberg eBook of Moby-Dick; or The Whale, by Herman Melville\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatsoever. You may copy it, give it away or re-use it under the terms\r\n",
            "of the Project Gutenberg License included with this eBook or online at\r\n",
            "www.gutenberg.org. If you are not located in the United States, you\r\n",
            "will have to check the laws of the country where you are loca"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 Word importance with TextRank"
      ],
      "metadata": {
        "id": "Or2HaUfnluQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing text"
      ],
      "metadata": {
        "id": "svXySYo73LI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing puntuation, numbers and applying lower-case transformation\n",
        "import re\n",
        "\n",
        "def preprocessing_string(text):\n",
        "  text = re.sub(r'\\S*@\\S*\\s?', '', text) #remove emails\n",
        "  text = re.sub('\\n\\n+','\\n', text)\n",
        "  text = re.sub(r'[^\\w\\s\\n]', ' ', text) #replaces not a word character (\\w) or a whitespace character (\\s) with a space\n",
        "  text = re.sub(r' +', ' ', text) #replaces multiple whitespaces with single whitespaces\n",
        "  text = re.sub(r'\\d+', '', text) #remove numbers\n",
        "  text = text.lower()\n",
        "  return text"
      ],
      "metadata": {
        "id": "2bDyMLe5iXis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('2701-0.txt', \"r\").read()\n",
        "print('Original text:\\n\\n',text[7000:7500])\n",
        "text_ = preprocessing_string(text[7000:])\n",
        "print('\\nPreprocessed text:\\n\\n',text_[:500])\n",
        "open('text.txt', \"w\").write(text_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1VmtDqy5z5r",
        "outputId": "ce8501bb-1def-4efd-89b5-d90db0aa988c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:\n",
            "\n",
            "             _Spanish_.\n",
            "  PEKEE-NUEE-NUEE,    _Fegee_.\n",
            "  PEHEE-NUEE-NUEE,    _Erromangoan_.\n",
            "\n",
            "\n",
            "\n",
            "  EXTRACTS. (Supplied by a Sub-Sub-Librarian).\n",
            "\n",
            "\n",
            "\n",
            "  It will be seen that this mere painstaking burrower and grub-worm of\n",
            "  a poor devil of a Sub-Sub appears to have gone through the long\n",
            "  Vaticans and street-stalls of the earth, picking up whatever random\n",
            "  allusions to whales he could anyways find in any book whatsoever,\n",
            "  sacred or profane. Therefore you must not, in every case at least,\n",
            "  take the h\n",
            "\n",
            "Preprocessed text:\n",
            "\n",
            "  _spanish_ \n",
            " pekee nuee nuee _fegee_ \n",
            " pehee nuee nuee _erromangoan_ \n",
            " extracts supplied by a sub sub librarian \n",
            " it will be seen that this mere painstaking burrower and grub worm of\n",
            " a poor devil of a sub sub appears to have gone through the long\n",
            " vaticans and street stalls of the earth picking up whatever random\n",
            " allusions to whales he could anyways find in any book whatsoever \n",
            " sacred or profane therefore you must not in every case at least \n",
            " take the higgledy piggledy whale statements howeve\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1194379"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving preprocessed text to HDFS"
      ],
      "metadata": {
        "id": "tK7mo_jKuT6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#putting the file from local file system to hadoop distributed file system\n",
        "!$HADOOP_HOME/bin/hdfs dfs -put /content/text.txt /TextRank"
      ],
      "metadata": {
        "id": "m-FX202zYRyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /TextRank"
      ],
      "metadata": {
        "id": "6UtWuJqoYWXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8dc0d4-3e95-4af8-efde-cfb11a7c6067"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 items\n",
            "-rw-r--r--   1 root supergroup    1194415 2023-05-29 07:33 /TextRank/text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mappper and reducer"
      ],
      "metadata": {
        "id": "kkYoczvF3JRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper1.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# Libraries\n",
        "import sys\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "N = int(sys.argv[1]) #window size\n",
        "\n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "  # to remove leading and trailing whitespace\n",
        "  line = line.strip()\n",
        "  # split the line into words\n",
        "  words = line.split() #white space by default\n",
        "\n",
        "  #Stopwords removal\n",
        "  words = [word for word in words if not word in stop_words]\n",
        "\n",
        "  for i in range(len(words)):\n",
        "\n",
        "    target = words[i]\n",
        "\n",
        "    for j in range(N):\n",
        "      if i-j >0: #this condition makes sure there are source words to retrieve\n",
        "        source = words[i-j-1]\n",
        "        print('%s\\t%s' % (target, source))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng_diQEoycRm",
        "outputId": "b9f957e6-b760-41eb-e7c1-2ebf3415cc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer1.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "threshold = int(sys.argv[1]) #minimum number of links a target word needs to have to appear in the output\n",
        "current_target = None\n",
        "current_source = None\n",
        "count = 1\n",
        "target = None\n",
        "\n",
        "# read the entire line from STDIN\n",
        "for line in sys.stdin:\n",
        "  line = line.strip() # remove leading and trailing whitespace\n",
        "  target, source = line.split('\\t', 1)\n",
        "\n",
        "  if current_target == target:\n",
        "    if current_source == source:\n",
        "      continue\n",
        "    else:\n",
        "      current_source = source\n",
        "      count += 1\n",
        "  else:\n",
        "    # write result to STDOUT\n",
        "    if count > threshold:\n",
        "      print('%s\\t%d' % (current_target, count))\n",
        "    current_source = source\n",
        "    current_target = target\n",
        "    count = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTLHLevnyhOL",
        "outputId": "d118b7c3-4386-4ca6-809d-fa46620da972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the python files work properly\n",
        "# !cat text.txt | python mapper1.py 1 | sort -k 1,1 | python reducer1.py 100"
      ],
      "metadata": {
        "id": "XLOPHLBJy93i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Changing the permissions of the python files\n",
        "!chmod 777 /content/mapper1.py /content/reducer1.py"
      ],
      "metadata": {
        "id": "5AFNKunOkYKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window size N = 1"
      ],
      "metadata": {
        "id": "vKjcDOa2t3xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Running hadoop streaming\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -D map.output.key.field.separator=\\t \\\n",
        "  -input /TextRank/text.txt \\\n",
        "  -output /TextRank/output1 \\\n",
        "  -mapper \"python /content/mapper1.py 1\" \\\n",
        "  -reducer \"python /content/reducer1.py 100\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCdcfcO1khgn",
        "outputId": "522d137e-2d0b-4e8a-cc70-e8c27210a81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar4324177563823712920/] [] /tmp/streamjob2632190798574981789.jar tmpDir=null\n",
            "2023-05-29 08:13:19,012 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-29 08:13:19,218 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-29 08:13:19,450 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1685347985977_0001\n",
            "2023-05-29 08:13:19,708 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-05-29 08:13:20,194 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2023-05-29 08:13:20,223 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
            "2023-05-29 08:13:20,748 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1685347985977_0001\n",
            "2023-05-29 08:13:20,749 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-05-29 08:13:20,926 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-05-29 08:13:20,926 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-05-29 08:13:21,189 INFO impl.YarnClientImpl: Submitted application application_1685347985977_0001\n",
            "2023-05-29 08:13:21,253 INFO mapreduce.Job: The url to track the job: http://de8bf04d0ad8:8088/proxy/application_1685347985977_0001/\n",
            "2023-05-29 08:13:21,254 INFO mapreduce.Job: Running job: job_1685347985977_0001\n",
            "2023-05-29 08:13:31,544 INFO mapreduce.Job: Job job_1685347985977_0001 running in uber mode : false\n",
            "2023-05-29 08:13:31,545 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-05-29 08:13:45,779 INFO mapreduce.Job:  map 50% reduce 0%\n",
            "2023-05-29 08:13:46,784 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-05-29 08:13:52,844 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-05-29 08:13:52,851 INFO mapreduce.Job: Job job_1685347985977_0001 completed successfully\n",
            "2023-05-29 08:13:52,933 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1458702\n",
            "\t\tFILE: Number of bytes written=3633113\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=1198693\n",
            "\t\tHDFS: Number of bytes written=997\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=24513\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=3586\n",
            "\t\tTotal time spent by all map tasks (ms)=24513\n",
            "\t\tTotal time spent by all reduce tasks (ms)=3586\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=24513\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=3586\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=25101312\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3672064\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=19036\n",
            "\t\tMap output records=93298\n",
            "\t\tMap output bytes=1272100\n",
            "\t\tMap output materialized bytes=1458708\n",
            "\t\tInput split bytes=182\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15723\n",
            "\t\tReduce shuffle bytes=1458708\n",
            "\t\tReduce input records=93298\n",
            "\t\tReduce output records=107\n",
            "\t\tSpilled Records=186596\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=729\n",
            "\t\tCPU time spent (ms)=4440\n",
            "\t\tPhysical memory (bytes) snapshot=824102912\n",
            "\t\tVirtual memory (bytes) snapshot=7609733120\n",
            "\t\tTotal committed heap usage (bytes)=844627968\n",
            "\t\tPeak Map Physical memory (bytes)=325189632\n",
            "\t\tPeak Map Virtual memory (bytes)=2535694336\n",
            "\t\tPeak Reduce Physical memory (bytes)=176836608\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2539716608\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1198511\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=997\n",
            "2023-05-29 08:13:52,933 INFO streaming.StreamJob: Output directory: /TextRank/output1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /TextRank/output1\n",
        "# !$HADOOP_HOME/bin/hdfs dfs -rm -r /TextRank/output1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9HsheUBkx4Z",
        "outputId": "b2f92b66-5fd2-40a0-d393-57dac9e9d203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2023-05-27 09:16 /TextRank/output1/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup        997 2023-05-27 09:16 /TextRank/output1/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the output from hadoop file system\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /TextRank/output1/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMCNyVRnmMhs",
        "outputId": "7daccf2b-9ad7-486c-b5ab-4d87ba11b292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahab\t423\n",
            "air\t119\n",
            "almost\t169\n",
            "among\t152\n",
            "away\t168\n",
            "aye\t104\n",
            "back\t140\n",
            "boat\t287\n",
            "boats\t110\n",
            "came\t109\n",
            "captain\t257\n",
            "come\t161\n",
            "could\t180\n",
            "crew\t117\n",
            "cried\t141\n",
            "day\t146\n",
            "deck\t159\n",
            "even\t162\n",
            "ever\t170\n",
            "every\t189\n",
            "eyes\t141\n",
            "far\t130\n",
            "feet\t104\n",
            "first\t180\n",
            "fish\t130\n",
            "found\t102\n",
            "full\t111\n",
            "go\t161\n",
            "god\t129\n",
            "good\t165\n",
            "great\t238\n",
            "half\t105\n",
            "hand\t178\n",
            "hands\t108\n",
            "head\t281\n",
            "know\t108\n",
            "last\t216\n",
            "let\t134\n",
            "life\t149\n",
            "like\t569\n",
            "line\t125\n",
            "little\t207\n",
            "long\t270\n",
            "look\t161\n",
            "made\t160\n",
            "man\t448\n",
            "many\t142\n",
            "mast\t107\n",
            "may\t217\n",
            "men\t219\n",
            "might\t159\n",
            "much\t183\n",
            "must\t236\n",
            "never\t172\n",
            "night\t123\n",
            "oh\t117\n",
            "old\t386\n",
            "one\t800\n",
            "part\t133\n",
            "pequod\t132\n",
            "place\t102\n",
            "queequeg\t196\n",
            "right\t130\n",
            "round\t214\n",
            "said\t265\n",
            "say\t191\n",
            "sea\t393\n",
            "see\t224\n",
            "seemed\t237\n",
            "seen\t136\n",
            "ship\t435\n",
            "side\t179\n",
            "sir\t142\n",
            "soon\t104\n",
            "sort\t129\n",
            "sperm\t196\n",
            "starbuck\t168\n",
            "still\t257\n",
            "stubb\t212\n",
            "take\t117\n",
            "tell\t103\n",
            "thee\t118\n",
            "thing\t156\n",
            "things\t106\n",
            "thou\t223\n",
            "though\t302\n",
            "thought\t118\n",
            "three\t197\n",
            "thus\t102\n",
            "till\t111\n",
            "time\t267\n",
            "two\t252\n",
            "upon\t503\n",
            "us\t203\n",
            "water\t155\n",
            "way\t231\n",
            "well\t179\n",
            "whale\t980\n",
            "whales\t220\n",
            "whaling\t104\n",
            "white\t226\n",
            "whole\t105\n",
            "without\t146\n",
            "world\t139\n",
            "would\t352\n",
            "ye\t404\n",
            "yet\t295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Threshold = 400"
      ],
      "metadata": {
        "id": "fNZt-djXaLKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Running hadoop streaming\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -D map.output.key.field.separator=\\t \\\n",
        "  -input /TextRank/text.txt \\\n",
        "  -output /TextRank/output400 \\\n",
        "  -mapper \"python /content/mapper1.py 1\" \\\n",
        "  -reducer \"python /content/reducer1.py 400\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4pxR-jYaXlA",
        "outputId": "b0be6af2-e438-423e-89bd-103c55c4562c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar8539352921727148282/] [] /tmp/streamjob5841608372509980963.jar tmpDir=null\n",
            "2023-05-29 08:13:54,865 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-29 08:13:55,076 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-29 08:13:55,298 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1685347985977_0002\n",
            "2023-05-29 08:13:55,955 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-05-29 08:13:56,437 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2023-05-29 08:13:56,478 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
            "2023-05-29 08:13:57,015 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1685347985977_0002\n",
            "2023-05-29 08:13:57,017 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-05-29 08:13:57,172 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-05-29 08:13:57,173 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-05-29 08:13:57,219 INFO impl.YarnClientImpl: Submitted application application_1685347985977_0002\n",
            "2023-05-29 08:13:57,252 INFO mapreduce.Job: The url to track the job: http://de8bf04d0ad8:8088/proxy/application_1685347985977_0002/\n",
            "2023-05-29 08:13:57,253 INFO mapreduce.Job: Running job: job_1685347985977_0002\n",
            "2023-05-29 08:14:05,436 INFO mapreduce.Job: Job job_1685347985977_0002 running in uber mode : false\n",
            "2023-05-29 08:14:05,438 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-05-29 08:14:14,544 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-05-29 08:14:19,581 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-05-29 08:14:20,597 INFO mapreduce.Job: Job job_1685347985977_0002 completed successfully\n",
            "2023-05-29 08:14:20,665 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1458702\n",
            "\t\tFILE: Number of bytes written=3633119\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=1198693\n",
            "\t\tHDFS: Number of bytes written=69\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=12937\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=2477\n",
            "\t\tTotal time spent by all map tasks (ms)=12937\n",
            "\t\tTotal time spent by all reduce tasks (ms)=2477\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=12937\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=2477\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=13247488\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2536448\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=19036\n",
            "\t\tMap output records=93298\n",
            "\t\tMap output bytes=1272100\n",
            "\t\tMap output materialized bytes=1458708\n",
            "\t\tInput split bytes=182\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15723\n",
            "\t\tReduce shuffle bytes=1458708\n",
            "\t\tReduce input records=93298\n",
            "\t\tReduce output records=8\n",
            "\t\tSpilled Records=186596\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=218\n",
            "\t\tCPU time spent (ms)=2880\n",
            "\t\tPhysical memory (bytes) snapshot=821571584\n",
            "\t\tVirtual memory (bytes) snapshot=7611592704\n",
            "\t\tTotal committed heap usage (bytes)=783286272\n",
            "\t\tPeak Map Physical memory (bytes)=323284992\n",
            "\t\tPeak Map Virtual memory (bytes)=2536210432\n",
            "\t\tPeak Reduce Physical memory (bytes)=175693824\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2539986944\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1198511\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=69\n",
            "2023-05-29 08:14:20,665 INFO streaming.StreamJob: Output directory: /TextRank/output400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the output from hadoop file system\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /TextRank/output400/part-00000\n",
        "# !$HADOOP_HOME/bin/hdfs dfs -rm -r /TextRank/output400"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulTbyMlmai7e",
        "outputId": "9a59db56-73a1-4a10-b3b2-4700f150fe5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahab\t423\n",
            "like\t569\n",
            "man\t448\n",
            "one\t800\n",
            "ship\t435\n",
            "upon\t503\n",
            "whale\t980\n",
            "ye\t404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Extending TextRank"
      ],
      "metadata": {
        "id": "DYg29KwTljJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window size N = 2"
      ],
      "metadata": {
        "id": "DY6AliT-ul4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Running hadoop streaming\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input /TextRank/text.txt \\\n",
        "  -output /TextRank/output2 \\\n",
        "  -mapper \"python /content/mapper1.py 2\" \\\n",
        "  -reducer \"python /content/reducer1.py 400\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67lfJ_FHsVtn",
        "outputId": "0a83d9ab-5f4b-447e-d9c1-1d81a12a705d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar5936669180632338866/] [] /tmp/streamjob9112917586342223028.jar tmpDir=null\n",
            "2023-05-29 08:14:22,651 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-29 08:14:22,863 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-29 08:14:23,083 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1685347985977_0003\n",
            "2023-05-29 08:14:23,297 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-05-29 08:14:23,362 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2023-05-29 08:14:23,517 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1685347985977_0003\n",
            "2023-05-29 08:14:23,518 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-05-29 08:14:23,698 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-05-29 08:14:23,698 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-05-29 08:14:23,760 INFO impl.YarnClientImpl: Submitted application application_1685347985977_0003\n",
            "2023-05-29 08:14:23,794 INFO mapreduce.Job: The url to track the job: http://de8bf04d0ad8:8088/proxy/application_1685347985977_0003/\n",
            "2023-05-29 08:14:23,795 INFO mapreduce.Job: Running job: job_1685347985977_0003\n",
            "2023-05-29 08:14:34,002 INFO mapreduce.Job: Job job_1685347985977_0003 running in uber mode : false\n",
            "2023-05-29 08:14:34,006 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-05-29 08:14:44,151 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-05-29 08:14:49,185 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-05-29 08:14:50,202 INFO mapreduce.Job: Job job_1685347985977_0003 completed successfully\n",
            "2023-05-29 08:14:50,311 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2622950\n",
            "\t\tFILE: Number of bytes written=5961200\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=1198693\n",
            "\t\tHDFS: Number of bytes written=257\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=16030\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=2716\n",
            "\t\tTotal time spent by all map tasks (ms)=16030\n",
            "\t\tTotal time spent by all reduce tasks (ms)=2716\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=16030\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=2716\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=16414720\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=2781184\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=19036\n",
            "\t\tMap output records=168012\n",
            "\t\tMap output bytes=2286920\n",
            "\t\tMap output materialized bytes=2622956\n",
            "\t\tInput split bytes=182\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15723\n",
            "\t\tReduce shuffle bytes=2622956\n",
            "\t\tReduce input records=168012\n",
            "\t\tReduce output records=28\n",
            "\t\tSpilled Records=336024\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=341\n",
            "\t\tCPU time spent (ms)=3820\n",
            "\t\tPhysical memory (bytes) snapshot=829931520\n",
            "\t\tVirtual memory (bytes) snapshot=7615205376\n",
            "\t\tTotal committed heap usage (bytes)=838336512\n",
            "\t\tPeak Map Physical memory (bytes)=330858496\n",
            "\t\tPeak Map Virtual memory (bytes)=2538934272\n",
            "\t\tPeak Reduce Physical memory (bytes)=177324032\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2540994560\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1198511\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=257\n",
            "2023-05-29 08:14:50,311 INFO streaming.StreamJob: Output directory: /TextRank/output2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /TextRank/output2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1Ha3BFMtPH1",
        "outputId": "b2f9e4c4-50e5-49d3-af90-8eab477d9827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2023-05-26 15:37 /TextRank/output2/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup       2254 2023-05-26 15:37 /TextRank/output2/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the output from hadoop file system\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /TextRank/output2/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPR0a3mPtSSo",
        "outputId": "79d7fad8-ef39-4db0-9213-94d9af9de11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahab\t808\n",
            "boat\t516\n",
            "captain\t457\n",
            "great\t428\n",
            "head\t532\n",
            "like\t1032\n",
            "long\t485\n",
            "man\t822\n",
            "men\t405\n",
            "must\t436\n",
            "old\t690\n",
            "one\t1454\n",
            "said\t450\n",
            "sea\t707\n",
            "seemed\t430\n",
            "ship\t788\n",
            "still\t462\n",
            "thou\t406\n",
            "though\t551\n",
            "time\t469\n",
            "two\t459\n",
            "upon\t930\n",
            "way\t420\n",
            "whale\t1826\n",
            "white\t414\n",
            "would\t645\n",
            "ye\t742\n",
            "yet\t535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Window size N = 3"
      ],
      "metadata": {
        "id": "rc30rRt-uqoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Running hadoop streaming\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -input /TextRank/text.txt \\\n",
        "  -output /TextRank/output3 \\\n",
        "  -mapper \"python /content/mapper1.py 3\" \\\n",
        "  -reducer \"python /content/reducer1.py 400\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8678TjGuvPR",
        "outputId": "a4b5aa4c-3fd7-4f0b-c17f-a8bac3612f39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar9160301363666113772/] [] /tmp/streamjob2696337336543597418.jar tmpDir=null\n",
            "2023-05-29 08:14:52,336 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-29 08:14:52,540 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-29 08:14:52,767 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1685347985977_0004\n",
            "2023-05-29 08:14:52,991 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-05-29 08:14:53,471 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2023-05-29 08:14:53,647 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1685347985977_0004\n",
            "2023-05-29 08:14:53,648 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-05-29 08:14:53,820 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-05-29 08:14:53,821 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-05-29 08:14:53,886 INFO impl.YarnClientImpl: Submitted application application_1685347985977_0004\n",
            "2023-05-29 08:14:53,945 INFO mapreduce.Job: The url to track the job: http://de8bf04d0ad8:8088/proxy/application_1685347985977_0004/\n",
            "2023-05-29 08:14:53,947 INFO mapreduce.Job: Running job: job_1685347985977_0004\n",
            "2023-05-29 08:15:01,151 INFO mapreduce.Job: Job job_1685347985977_0004 running in uber mode : false\n",
            "2023-05-29 08:15:01,152 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-05-29 08:15:10,283 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-05-29 08:15:17,359 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-05-29 08:15:17,368 INFO mapreduce.Job: Job job_1685347985977_0004 completed successfully\n",
            "2023-05-29 08:15:17,453 INFO mapreduce.Job: Counters: 55\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3502401\n",
            "\t\tFILE: Number of bytes written=7720102\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=1198693\n",
            "\t\tHDFS: Number of bytes written=519\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tKilled map tasks=1\n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=13204\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=4464\n",
            "\t\tTotal time spent by all map tasks (ms)=13204\n",
            "\t\tTotal time spent by all reduce tasks (ms)=4464\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=13204\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=4464\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=13520896\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=4571136\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=19036\n",
            "\t\tMap output records=224639\n",
            "\t\tMap output bytes=3053117\n",
            "\t\tMap output materialized bytes=3502407\n",
            "\t\tInput split bytes=182\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=15723\n",
            "\t\tReduce shuffle bytes=3502407\n",
            "\t\tReduce input records=224639\n",
            "\t\tReduce output records=55\n",
            "\t\tSpilled Records=449278\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=314\n",
            "\t\tCPU time spent (ms)=3750\n",
            "\t\tPhysical memory (bytes) snapshot=778817536\n",
            "\t\tVirtual memory (bytes) snapshot=7612592128\n",
            "\t\tTotal committed heap usage (bytes)=805830656\n",
            "\t\tPeak Map Physical memory (bytes)=281096192\n",
            "\t\tPeak Map Virtual memory (bytes)=2536841216\n",
            "\t\tPeak Reduce Physical memory (bytes)=218279936\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2541019136\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1198511\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=519\n",
            "2023-05-29 08:15:17,454 INFO streaming.StreamJob: Output directory: /TextRank/output3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /TextRank/output3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIU8zoTtvivs",
        "outputId": "a8544144-adb5-446b-b9df-3424cabc6d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2023-05-26 15:47 /TextRank/output3/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup       3149 2023-05-26 15:47 /TextRank/output3/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the output from hadoop file system\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /TextRank/output3/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iu8nPvZvmC6",
        "outputId": "14143a2e-0e6c-4ab0-d435-3549df7fe9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahab\t1071\n",
            "almost\t407\n",
            "away\t415\n",
            "boat\t698\n",
            "captain\t605\n",
            "could\t453\n",
            "deck\t404\n",
            "ever\t417\n",
            "every\t462\n",
            "first\t435\n",
            "go\t412\n",
            "great\t557\n",
            "hand\t470\n",
            "head\t726\n",
            "last\t527\n",
            "like\t1397\n",
            "little\t492\n",
            "long\t652\n",
            "made\t401\n",
            "man\t1090\n",
            "may\t509\n",
            "men\t538\n",
            "much\t440\n",
            "must\t600\n",
            "never\t422\n",
            "old\t912\n",
            "one\t1940\n",
            "queequeg\t449\n",
            "round\t521\n",
            "said\t582\n",
            "say\t477\n",
            "sea\t963\n",
            "see\t535\n",
            "seemed\t578\n",
            "ship\t1046\n",
            "side\t453\n",
            "sperm\t476\n",
            "starbuck\t425\n",
            "still\t623\n",
            "stubb\t529\n",
            "thou\t540\n",
            "though\t741\n",
            "three\t471\n",
            "time\t623\n",
            "two\t615\n",
            "upon\t1287\n",
            "us\t525\n",
            "way\t546\n",
            "well\t426\n",
            "whale\t2439\n",
            "whales\t516\n",
            "white\t555\n",
            "would\t875\n",
            "ye\t998\n",
            "yet\t708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting files form HDFS\n",
        "!$HADOOP_HOME/bin/hdfs dfs -get /TextRank/output400/part-00000 /content/output400.txt\n",
        "!$HADOOP_HOME/bin/hdfs dfs -get /TextRank/output2/part-00000 /content/output2.txt\n",
        "!$HADOOP_HOME/bin/hdfs dfs -get /TextRank/output3/part-00000 /content/output3.txt"
      ],
      "metadata": {
        "id": "rGNWa6MCtg8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "e0CfwFsFhhBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Coverting outputs in data frames\n",
        "df1 = pd.read_csv('output400.txt', sep='\\t', header=None, index_col=0)\n",
        "df2 = pd.read_csv('output2.txt', sep='\\t', header=None, index_col=0)\n",
        "df3 = pd.read_csv('output3.txt', sep='\\t', header=None, index_col=0)"
      ],
      "metadata": {
        "id": "bFy8PUAhguLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qualitative analysis\n",
        "\n",
        "As expected, the bigger the window size, the higher the number of links. However, the incremental percentage is similar for all the words, meaning that the relative importance of the words and the ranking remains the same.\n",
        "\n",
        "Note 1: this implementation does not take into consideration the sentences as containers for link counting. Naturally, there should not be links between words of different sentences.\n",
        "\n",
        "Note 2: since the Mapper reads the file by line, the first words in the lines are misrepresented. That is, first target words in the line do not have source words from the previous line, and therefore they are misrepresented. However, this situation affects all the words randomly and should not influence the relative importance of the words.\n"
      ],
      "metadata": {
        "id": "0Zokq3fHogY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 Engineering Task"
      ],
      "metadata": {
        "id": "Z0Z1aW2qla4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-stage MapReduce"
      ],
      "metadata": {
        "id": "y4nCjYeRRAUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper2.py\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "  line = line.strip()\n",
        "\n",
        "  word, count = line.split('\\t', 1)\n",
        "\n",
        "  print('%s\\t%s' % (count, word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzBI_1xuQ_x8",
        "outputId": "c0ae9baa-6338-4c88-d345-ac02c03bd3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Running hadoop streaming\n",
        "!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \\\n",
        "  -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
        "  -D mapred.text.key.comparator.options=-k1,1nr \\\n",
        "  -input /TextRank/output1/part-00000 \\\n",
        "  -output /TextRank/output4 \\\n",
        "  -mapper \"python /content/mapper2.py\" \\\n",
        "  -reducer org.apache.hadoop.mapred.lib.IdentityReducer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnIqciJbyf4i",
        "outputId": "63ba5af3-ce04-4989-ce03-258e929a4859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packageJobJar: [/tmp/hadoop-unjar7135053949676661382/] [] /tmp/streamjob9048920344804256703.jar tmpDir=null\n",
            "2023-05-27 09:17:39,519 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-27 09:17:39,867 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n",
            "2023-05-27 09:17:40,256 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1685178922525_0002\n",
            "2023-05-27 09:17:40,601 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2023-05-27 09:17:41,088 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2023-05-27 09:17:41,135 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
            "2023-05-27 09:17:41,136 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
            "2023-05-27 09:17:41,304 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1685178922525_0002\n",
            "2023-05-27 09:17:41,306 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2023-05-27 09:17:41,594 INFO conf.Configuration: resource-types.xml not found\n",
            "2023-05-27 09:17:41,596 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "2023-05-27 09:17:41,696 INFO impl.YarnClientImpl: Submitted application application_1685178922525_0002\n",
            "2023-05-27 09:17:41,744 INFO mapreduce.Job: The url to track the job: http://252b90078f63:8088/proxy/application_1685178922525_0002/\n",
            "2023-05-27 09:17:41,746 INFO mapreduce.Job: Running job: job_1685178922525_0002\n",
            "2023-05-27 09:17:51,956 INFO mapreduce.Job: Job job_1685178922525_0002 running in uber mode : false\n",
            "2023-05-27 09:17:51,957 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2023-05-27 09:18:02,171 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2023-05-27 09:18:08,224 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2023-05-27 09:18:09,242 INFO mapreduce.Job: Job job_1685178922525_0002 completed successfully\n",
            "2023-05-27 09:18:09,368 INFO mapreduce.Job: Counters: 54\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1217\n",
            "\t\tFILE: Number of bytes written=717183\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\t\tHDFS: Number of bytes read=1698\n",
            "\t\tHDFS: Number of bytes written=997\n",
            "\t\tHDFS: Number of read operations=11\n",
            "\t\tHDFS: Number of large read operations=0\n",
            "\t\tHDFS: Number of write operations=2\n",
            "\t\tHDFS: Number of bytes read erasure-coded=0\n",
            "\tJob Counters \n",
            "\t\tLaunched map tasks=2\n",
            "\t\tLaunched reduce tasks=1\n",
            "\t\tData-local map tasks=2\n",
            "\t\tTotal time spent by all maps in occupied slots (ms)=16066\n",
            "\t\tTotal time spent by all reduces in occupied slots (ms)=3061\n",
            "\t\tTotal time spent by all map tasks (ms)=16066\n",
            "\t\tTotal time spent by all reduce tasks (ms)=3061\n",
            "\t\tTotal vcore-milliseconds taken by all map tasks=16066\n",
            "\t\tTotal vcore-milliseconds taken by all reduce tasks=3061\n",
            "\t\tTotal megabyte-milliseconds taken by all map tasks=16451584\n",
            "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3134464\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=107\n",
            "\t\tMap output records=107\n",
            "\t\tMap output bytes=997\n",
            "\t\tMap output materialized bytes=1223\n",
            "\t\tInput split bytes=202\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=82\n",
            "\t\tReduce shuffle bytes=1223\n",
            "\t\tReduce input records=107\n",
            "\t\tReduce output records=107\n",
            "\t\tSpilled Records=214\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=642\n",
            "\t\tCPU time spent (ms)=2270\n",
            "\t\tPhysical memory (bytes) snapshot=865775616\n",
            "\t\tVirtual memory (bytes) snapshot=7613808640\n",
            "\t\tTotal committed heap usage (bytes)=871890944\n",
            "\t\tPeak Map Physical memory (bytes)=322887680\n",
            "\t\tPeak Map Virtual memory (bytes)=2537123840\n",
            "\t\tPeak Reduce Physical memory (bytes)=221089792\n",
            "\t\tPeak Reduce Virtual memory (bytes)=2540933120\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1496\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=997\n",
            "2023-05-27 09:18:09,368 INFO streaming.StreamJob: Output directory: /TextRank/output4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploring hadoop folder\n",
        "!$HADOOP_HOME/bin/hdfs dfs -ls /TextRank/output4\n",
        "# !$HADOOP_HOME/bin/hdfs dfs -rm -r /TextRank/output4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOR9uGJzzFrS",
        "outputId": "4328648b-7d33-4fd7-e788-c8580d8447b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root supergroup          0 2023-05-27 09:18 /TextRank/output4/_SUCCESS\n",
            "-rw-r--r--   1 root supergroup        997 2023-05-27 09:18 /TextRank/output4/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the output from hadoop file system\n",
        "!$HADOOP_HOME/bin/hdfs dfs -cat /TextRank/output4/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qND_PMdtzK6b",
        "outputId": "991a6be2-c4e1-4fa4-e5ee-7f9dee083865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "980\twhale\n",
            "800\tone\n",
            "569\tlike\n",
            "503\tupon\n",
            "448\tman\n",
            "435\tship\n",
            "423\tahab\n",
            "404\tye\n",
            "393\tsea\n",
            "386\told\n",
            "352\twould\n",
            "302\tthough\n",
            "295\tyet\n",
            "287\tboat\n",
            "281\thead\n",
            "270\tlong\n",
            "267\ttime\n",
            "265\tsaid\n",
            "257\tstill\n",
            "257\tcaptain\n",
            "252\ttwo\n",
            "238\tgreat\n",
            "237\tseemed\n",
            "236\tmust\n",
            "231\tway\n",
            "226\twhite\n",
            "224\tsee\n",
            "223\tthou\n",
            "220\twhales\n",
            "219\tmen\n",
            "217\tmay\n",
            "216\tlast\n",
            "214\tround\n",
            "212\tstubb\n",
            "207\tlittle\n",
            "203\tus\n",
            "197\tthree\n",
            "196\tsperm\n",
            "196\tqueequeg\n",
            "191\tsay\n",
            "189\tevery\n",
            "183\tmuch\n",
            "180\tfirst\n",
            "180\tcould\n",
            "179\tside\n",
            "179\twell\n",
            "178\thand\n",
            "172\tnever\n",
            "170\tever\n",
            "169\talmost\n",
            "168\taway\n",
            "168\tstarbuck\n",
            "165\tgood\n",
            "162\teven\n",
            "161\tgo\n",
            "161\tlook\n",
            "161\tcome\n",
            "160\tmade\n",
            "159\tmight\n",
            "159\tdeck\n",
            "156\tthing\n",
            "155\twater\n",
            "152\tamong\n",
            "149\tlife\n",
            "146\tday\n",
            "146\twithout\n",
            "142\tsir\n",
            "142\tmany\n",
            "141\teyes\n",
            "141\tcried\n",
            "140\tback\n",
            "139\tworld\n",
            "136\tseen\n",
            "134\tlet\n",
            "133\tpart\n",
            "132\tpequod\n",
            "130\tright\n",
            "130\tfar\n",
            "130\tfish\n",
            "129\tgod\n",
            "129\tsort\n",
            "125\tline\n",
            "123\tnight\n",
            "119\tair\n",
            "118\tthee\n",
            "118\tthought\n",
            "117\toh\n",
            "117\ttake\n",
            "117\tcrew\n",
            "111\tfull\n",
            "111\ttill\n",
            "110\tboats\n",
            "109\tcame\n",
            "108\tknow\n",
            "108\thands\n",
            "107\tmast\n",
            "106\tthings\n",
            "105\twhole\n",
            "105\thalf\n",
            "104\tfeet\n",
            "104\taye\n",
            "104\tsoon\n",
            "104\twhaling\n",
            "103\ttell\n",
            "102\tplace\n",
            "102\tthus\n",
            "102\tfound\n"
          ]
        }
      ]
    }
  ]
}