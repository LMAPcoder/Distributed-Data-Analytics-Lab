{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsrF7IMVCxeHev/ONqdBTI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab Distributed Data Analytics**"
      ],
      "metadata": {
        "id": "uCTM-UseEKqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tutorial 2"
      ],
      "metadata": {
        "id": "9loVfcz_ELws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying information about the CPU architecture\n",
        "!lscpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWF5QIA5a6I4",
        "outputId": "ef537e9e-a59b-4c05-ee73-27baa59b0bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:                    x86_64\n",
            "CPU op-mode(s):                  32-bit, 64-bit\n",
            "Byte Order:                      Little Endian\n",
            "Address sizes:                   46 bits physical, 48 bits virtual\n",
            "CPU(s):                          2\n",
            "On-line CPU(s) list:             0,1\n",
            "Thread(s) per core:              2\n",
            "Core(s) per socket:              1\n",
            "Socket(s):                       1\n",
            "NUMA node(s):                    1\n",
            "Vendor ID:                       GenuineIntel\n",
            "CPU family:                      6\n",
            "Model:                           79\n",
            "Model name:                      Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Stepping:                        0\n",
            "CPU MHz:                         2199.998\n",
            "BogoMIPS:                        4399.99\n",
            "Hypervisor vendor:               KVM\n",
            "Virtualization type:             full\n",
            "L1d cache:                       32 KiB\n",
            "L1i cache:                       32 KiB\n",
            "L2 cache:                        256 KiB\n",
            "L3 cache:                        55 MiB\n",
            "NUMA node0 CPU(s):               0,1\n",
            "Vulnerability Itlb multihit:     Not affected\n",
            "Vulnerability L1tf:              Mitigation; PTE Inversion\n",
            "Vulnerability Mds:               Vulnerable; SMT Host state unknown\n",
            "Vulnerability Meltdown:          Vulnerable\n",
            "Vulnerability Mmio stale data:   Vulnerable\n",
            "Vulnerability Retbleed:          Vulnerable\n",
            "Vulnerability Spec store bypass: Vulnerable\n",
            "Vulnerability Spectre v1:        Vulnerable: __user pointer sanitization and use\n",
            "                                 rcopy barriers only; no swapgs barriers\n",
            "Vulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PB\n",
            "                                 RSB-eIBRS: Not affected\n",
            "Vulnerability Srbds:             Not affected\n",
            "Vulnerability Tsx async abort:   Vulnerable\n",
            "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtr\n",
            "                                 r pge mca cmov pat pse36 clflush mmx fxsr sse s\n",
            "                                 se2 ss ht syscall nx pdpe1gb rdtscp lm constant\n",
            "                                 _tsc rep_good nopl xtopology nonstop_tsc cpuid \n",
            "                                 tsc_known_freq pni pclmulqdq ssse3 fma cx16 pci\n",
            "                                 d sse4_1 sse4_2 x2apic movbe popcnt aes xsave a\n",
            "                                 vx f16c rdrand hypervisor lahf_lm abm 3dnowpref\n",
            "                                 etch invpcid_single ssbd ibrs ibpb stibp fsgsba\n",
            "                                 se tsc_adjust bmi1 hle avx2 smep bmi2 erms invp\n",
            "                                 cid rtm rdseed adx smap xsaveopt arat md_clear \n",
            "                                 arch_capabilities\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.cpu_count() #number of logical CPU cores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmKRhqT-4smp",
        "outputId": "34133f38-dc13-475e-c844-3c29d7e99e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Verifying open mpi installation\n",
        "!ompi_info --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41PW8jabaFiy",
        "outputId": "ce626496-01be-4ba5-c4d2-8f33782a4f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Open MPI v4.0.3\n",
            "\n",
            "http://www.open-mpi.org/community/help/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing python library mpi4py\n",
        "!pip install --quiet mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k42krZ-2ZdNa",
        "outputId": "4debf166-79d0-409c-deb2-e4ad2ecae2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/2.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m1.9/2.5 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 Vector Multiplication"
      ],
      "metadata": {
        "id": "7c8a7EAkZev7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7rpcvDbZLcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa0df14-ab38-4291-b991-280c58e6c618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ex1.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ex1.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "comm = MPI.COMM_WORLD                   #communicator\n",
        "rank = comm.Get_rank()                  #number of the process running the code\n",
        "size = comm.Get_size()                  #total number of processes running\n",
        "myHostName = MPI.Get_processor_name()   #machine name running the code\n",
        "\n",
        "t_0 = time.time() #start time\n",
        "\n",
        "def dot_product(N):\n",
        "\n",
        "  if size == 1:\n",
        "\n",
        "    a = np.arange(N)\n",
        "    b = np.arange(N, N*2)\n",
        "\n",
        "    #Element-wise product\n",
        "    sum = 0\n",
        "    for i in range(N):\n",
        "      sum += a[i] * b[i]\n",
        "\n",
        "    t_1 = time.time()\n",
        "    print(f'Time: {t_1-t_0:.5f}') #timing\n",
        "    return sum\n",
        "\n",
        "  else:\n",
        "    if rank == 0: #master/coordinator node\n",
        "      a = np.arange(N)\n",
        "      b = np.arange(N, N*2)\n",
        "      list1 = np.array_split(a, size, axis=0)\n",
        "      list2 = np.array_split(b, size, axis=0)\n",
        "\n",
        "      #Send method for buffer-like objects like numpy array\n",
        "      for i in range(1,size):\n",
        "        comm.send(list1[i], dest=i)\n",
        "        comm.send(list2[i], dest=i)\n",
        "\n",
        "      a_i = list1[0]\n",
        "      b_i = list2[0]\n",
        "\n",
        "      sum = 0\n",
        "      for i in range(len(a_i)):\n",
        "        sum += a_i[i] * b_i[i]\n",
        "\n",
        "      for i in range(1,size):\n",
        "          sum += comm.recv(source=i)\n",
        "\n",
        "      t_1 = time.time()\n",
        "      print(f'Time: {t_1-t_0:.5f}') #timing\n",
        "\n",
        "      return sum\n",
        "\n",
        "    else: #worker node\n",
        "      #Receive method for buffer-like objects like numpy array\n",
        "      a_i = comm.recv(source=0)\n",
        "      b_i = comm.recv(source=0)\n",
        "      sum = 0\n",
        "      for i in range(len(a_i)):\n",
        "        sum += a_i[i] * b_i[i]\n",
        "      comm.send(sum, dest=0)\n",
        "\n",
        "if rank == 0:\n",
        "  print(f'Cores: {size}')\n",
        "\n",
        "Ns = [6,12,24]\n",
        "for N in Ns:\n",
        "  sum = dot_product(N)\n",
        "  if rank == 0:\n",
        "    print(f'N={N}, Sum={sum}')\n",
        "\n",
        "MPI.Finalize()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --allow-run-as-root --use-hwthread-cpus --oversubscribe -n 1 python ex1.py\n",
        "! mpirun --allow-run-as-root --use-hwthread-cpus --oversubscribe -n 2 python ex1.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFJrTXyvbY-l",
        "outputId": "a2c49197-4640-4d99-8ada-cb5b20ed439c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cores: 1\n",
            "Time: 0.00010\n",
            "N=6, Sum=145\n",
            "Time: 0.00015\n",
            "N=12, Sum=1298\n",
            "Time: 0.00021\n",
            "N=24, Sum=10948\n",
            "Time: 0.00028\n",
            "N=100, Sum=823350\n",
            "Cores: 2\n",
            "Time: 0.00511\n",
            "N=6, Sum=145\n",
            "Time: 0.00537\n",
            "N=12, Sum=1298\n",
            "Time: 0.01009\n",
            "N=24, Sum=10948\n",
            "Time: 0.01038\n",
            "N=100, Sum=823350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 Collective Communication"
      ],
      "metadata": {
        "id": "d4SeGgayZmpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ex2.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "comm = MPI.COMM_WORLD                   #communicator\n",
        "rank = comm.Get_rank()                  #number of the process running the code\n",
        "size = comm.Get_size()                  #total number of processes running\n",
        "myHostName = MPI.Get_processor_name()   #machine name running the code\n",
        "\n",
        "t_0 = time.time() #start time\n",
        "\n",
        "def dot_product(N):\n",
        "\n",
        "  if size == 1:\n",
        "    a = np.arange(N)\n",
        "    b = np.arange(N, N*2)\n",
        "    #Element-wise product\n",
        "    sum = 0\n",
        "    for i in range(N):\n",
        "      sum += a[i] * b[i]\n",
        "\n",
        "    t_1 = time.time()\n",
        "    print(f'Time: {t_1-t_0:.5f}') #timing\n",
        "    return sum\n",
        "\n",
        "  else:\n",
        "    list1 = None\n",
        "    list2 = None\n",
        "\n",
        "    if rank == 0: #master node\n",
        "      a = np.arange(N)\n",
        "      b = np.arange(N, N*2)\n",
        "\n",
        "      list1 = np.array_split(a, size, axis=0)\n",
        "      list2 = np.array_split(b, size, axis=0)\n",
        "\n",
        "    #Distributing the data with collective communication\n",
        "\n",
        "    a_i = comm.scatter(list1, root=0)\n",
        "    b_i = comm.scatter(list2, root=0)\n",
        "\n",
        "    sum = 0\n",
        "    for i in range(len(a_i)):\n",
        "      sum += a_i[i] * b_i[i]\n",
        "\n",
        "    sum_p = comm.gather(sum, root=0)\n",
        "\n",
        "    if rank == 0: #master node\n",
        "      sum = 0\n",
        "      for s in sum_p:\n",
        "          sum += s\n",
        "\n",
        "      t_1 = time.time()\n",
        "      print(f'Time: {t_1-t_0:.5f}') #timing\n",
        "\n",
        "      return sum\n",
        "\n",
        "if rank == 0:\n",
        "  print(f'Cores: {size}')\n",
        "\n",
        "Ns = [6,12,24]\n",
        "for N in Ns:\n",
        "  sum = dot_product(N)\n",
        "  if rank == 0:\n",
        "    print(f'N={N}, Sum={sum}')\n",
        "\n",
        "MPI.Finalize()"
      ],
      "metadata": {
        "id": "8PHiI_CyZpPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d29adab-ab93-4d1f-80ba-0bd91f07cfa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ex2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --allow-run-as-root --use-hwthread-cpus --oversubscribe -n 1 python ex2.py\n",
        "! mpirun --allow-run-as-root --use-hwthread-cpus --oversubscribe -n 2 python ex2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fz_MwMxgc8j",
        "outputId": "c10174e2-c290-4685-fbb9-fe44896a314d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cores: 1\n",
            "Time: 0.00014\n",
            "N=6, Sum=145\n",
            "Time: 0.00023\n",
            "N=12, Sum=1298\n",
            "Time: 0.00032\n",
            "N=24, Sum=10948\n",
            "Cores: 2\n",
            "Time: 0.06977\n",
            "N=6, Sum=145\n",
            "Time: 0.07019\n",
            "N=12, Sum=1298\n",
            "Time: 0.07052\n",
            "N=24, Sum=10948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3 Distributed Sorting"
      ],
      "metadata": {
        "id": "BdUYDi0FZpqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ex3.py\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "\n",
        "comm = MPI.COMM_WORLD                   #communicator\n",
        "rank = comm.Get_rank()                  #number of the process running the code\n",
        "size = comm.Get_size()                  #total number of processes running\n",
        "myHostName = MPI.Get_processor_name()   #machine name running the code\n",
        "\n",
        "t_0 = time.time() #start time\n",
        "\n",
        "def sorting(N):\n",
        "\n",
        "  if size == 1:\n",
        "    my_list = list(range(N))\n",
        "    random.shuffle(my_list)\n",
        "\n",
        "    my_list.sort()\n",
        "\n",
        "    t_1 = time.time()\n",
        "    print(f'Time: {t_1-t_0:.5f}') #timing\n",
        "\n",
        "    return my_list\n",
        "\n",
        "  else:\n",
        "\n",
        "    sub_lists = None\n",
        "\n",
        "    if rank == 0: #master node\n",
        "      my_list = list(range(N))\n",
        "      random.shuffle(my_list)\n",
        "      sub_lists = np.array_split(my_list, size, axis=0)\n",
        "\n",
        "    #Distributing the data with collective communication\n",
        "\n",
        "    sub_list = comm.scatter(sub_lists, root=0)\n",
        "\n",
        "    sub_list.sort()\n",
        "\n",
        "    sub_lists = comm.gather(sub_list, root=0)\n",
        "\n",
        "    if rank == 0: #master node\n",
        "      my_list = list()\n",
        "      while len(my_list) < N:\n",
        "        n = N+1\n",
        "        j = 0\n",
        "        for i,sl in enumerate(sub_lists):\n",
        "          if len(sl) > 0:\n",
        "            if sl[0] < n:\n",
        "              n = sl[0]\n",
        "              j = i\n",
        "        sub_lists[j] = sub_lists[j][1:]\n",
        "        my_list.append(n)\n",
        "\n",
        "      t_1 = time.time()\n",
        "      print(f'Time: {t_1-t_0:.5f}') #timing\n",
        "\n",
        "      return my_list\n",
        "\n",
        "if rank == 0:\n",
        "  print(f'Cores: {size}')\n",
        "\n",
        "Ns = [10000,100000]\n",
        "for N in Ns:\n",
        "  my_list = sorting(N)\n",
        "  if rank == 0:\n",
        "    print(f'N={N}, Sorted list: {my_list[:5]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1cRur5ciLyB",
        "outputId": "6597b4ab-5410-456b-a014-5f82580d8763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ex3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun --allow-run-as-root --use-hwthread-cpus -n 1 python ex3.py\n",
        "! mpirun --allow-run-as-root --use-hwthread-cpus -n 2 python ex3.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoJCDUZXZ4f5",
        "outputId": "5b565b9e-3ac7-4704-e5e0-0db4363e39c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cores: 1\n",
            "Time: 0.00519\n",
            "N=10000, Sorted list: [0, 1, 2, 3, 4]\n",
            "Time: 0.06436\n",
            "N=100000, Sorted list: [0, 1, 2, 3, 4]\n",
            "Cores: 2\n",
            "Time: 0.03435\n",
            "N=10000, Sorted list: [0, 1, 2, 3, 4]\n",
            "Time: 0.29120\n",
            "N=100000, Sorted list: [0, 1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion: None of the exercises showed reductions in time when parallelizing the executions. The reason could be the communication time is longer than the code execution per se."
      ],
      "metadata": {
        "id": "zFzmSXf46nEi"
      }
    }
  ]
}